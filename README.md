# Italian-models-attention-analysis
Exploration of coreference relation in four Italian Language Models, with specific focus on differences between present and dropped mentions. 


Abstract (Master's thesis)

Large Language Models have recently experienced an enormous scaling up in size, which went hand in hand with improvements in performance. However, while LLMs have been at the centre of a wave of recent research, their inner workings are often still obscure. Moreover, since research mainly focused on languages which present an abundance of data, many major languages are still often left out of mainstream research. For this reason, this work attempts to explore Italian, a language which has been explored only a few times in LLMs research. This study explores the behaviour of four Italian models when confronted with the linguistic phenomenon of coreference, with specific observations focusing on the differences created by the possibility of pro-drop, a syntactic phenomenon typical of a few languages, among which figures Italian. The models are analysed using attention matrices which allow for the visualisation of patterns of attention activation, extracting cosine similarity scores which exploit closeness in vector space to register how similar two words are taken to be, and measuring the models' performances by training a probing classifier on the representations extracted from different points of their architectures. 

The observations stemming from the experiments are carried out in order to better understand how well and where the models attend to the linguistic relation studied. Moreover, the confrontation of the results coming from the different experiments is carried out in order to find correlations between model performance and models' behaviours for what concerns the activation of attention and the placement of token vectors in vector space. Two major observations coming from the analysis of the experiments' results are that cosine similarity scores seem to be a reflection of models' performances, and that the patterns of attention activation could be used to predict where, in a model, a specific linguistic phenomena is encoded, thus allowing to extract better representations which would result in a better performance by a classifier trained on them.
